aperm(aa, c(1,3,2))
alpha
array(y, c(TT, N, G))
y
xxb <- array(apply(xx, 3, function(z){beta %*% z}), c(TT, N, G))
xx
apply(xx, 3, function(z){beta %*% z})
beta
beta = b
xxb <- array(apply(xx, 3, function(z){b %*% z}), c(TT, N, G))
xxb
aa <- aperm(array(alpha, c(TT, G, N)), c(1, 3, 2))
aa
(yy - xxb - aa)^2
yy
yy <- array(y, c(TT, N, G))
yy
(yy - xxb - aa)^2
apply((yy - xxb - aa)^2, 2, sum)
apply((yy - xxb - aa)^2, c(1,3,2), sum)
apply((yy - xxb - aa)^2, c(1,2,3), sum)
yy
apply((yy - xxb - aa)^2, c(1,3), sum)
apply((yy - xxb - aa)^2, c(2,3), sum)
(yy - xxb - aa)^2
rss <- apply((yy - xxb - aa)^2, c(2,3), sum)
rss
apply(rss, 1, which.min)
g_ind <- apply(rss, 1, which.min)
g_ind
diagonal(3)
eye(3)
Diagonal(3)
diag(3)
D_t <- kronecker(rep(1,N), diag(TT))
TT
D_t
g
DD <- matrix(0, 12, 3)
DD
DD == Dt
DD == D_t
sum( DD == D_t )
sum( DD != D_t )
sample(1:8,2)
sample(1:8,2)
sample(1:8,2)
sample(1:8,2)
sample(1:8,2)
numeric(10)
a
a[c(3,1),]
a[c(3,1),2] <- c(1.1,3.1)
a
1:G
1:10[-3]
(1:10)[-3]
a <- c(5, 5, 6, 4, 4, 2, 4, 6, 6, 8, 5, 6, 4, 4, 3, 8, 5, 3, 6, 6)
sum(a)
length(a)
load("C:/Users/zhang/Dropbox/Research/panel_heterogeneity/computing/G10_dgp1_r1_10.RData")
colMeans(time_record)
colMeans(time_record)/60
load("C:/Users/zhang/Dropbox/Research/panel_heterogeneity/computing/G3_dgp1_r1_10.RData")
colMeans(time_record)/60
load("C:/Users/zhang/Dropbox/Research/panel_heterogeneity/computing/G3_dgp_benchmark.RData")
colMeans(time_record)/60
load("C:/Users/zhang/Dropbox/Research/panel_heterogeneity/computing/G10_dgp_benchmark.RData")
colMeans(time_record)/60
x <- matrix(rnorm(200), 100, 2)
y <- x%*% c(1,2) + rnorm(100)
x
y
y <- 1 + x%*% c(1,2) + rnorm(100)
lm(y ~ x)
lm(y ~ x)
x <- matrix(rnorm(200), 100, 10)
y <- 1 + x%*% rep(1, 10) + rnorm(100)
lm(y ~ x)
x <- matrix(rnorm(1000), 100, 10)
y <- 1 + x %*%  + rnorm(100)
lm(y ~ x)
x <- matrix(rnorm(1000), 100, 10)
y <- 1 + x %*% rep(1,10) + rnorm(100)
lm(y ~ x)
library("CVXR")
install.packages("CVXR")
CVXR::installed_solvers()
library("Rmosek")
source('C:/Program Files/Mosek/9.0/tools/platform/win64x86/rmosek/builder.R')
attachbuilder()
install.rmosek()
CVXR::installed_solvers()
n <- 100
p <- 1000
b0 <- c(1, -0.5, 0.7, -1.2, -0.9, 0.3, 0.55)
b0 <- c(b0, rep(0, p - length(b0)))
?mvtnorm::rmvnorm
mvtnorm::rmvnorm(10)
mvtnorm::rmvnorm(n = 10, mean = rep(0, 10))
mvtnorm::rmvnorm(n = 1, mean = rep(0, 10))
mvtnorm::rmvnorm(n = 2, mean = rep(0, 10))
install.packages("ncvreg")
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
load("QuickStartExample.RData")
y = x %*% c(rep(1,4), rep(0,16)) + rnorm(100)
x <- matrix(rnorm(2000), 100, 20)
y = x %*% c(rep(1,4), rep(0,16)) + rnorm(100)
fit = glmnet(x, y)
fit$df
fit$beta
fit$beta[, fit$df == 4]
x <- matrix(rnorm(100000), 100, 1000)
y = x %*% c(rep(1,7), rep(0,993)) + rnorm(100)
fit = glmnet(x, y)
fit$beta[, fit$df == 7]
load("QuickStartExample.RData")
x <- matrix(rnorm(100000), 100, 1000)
y = x %*% c(rep(0.2,7), rep(0,993)) + rnorm(100)
fit = glmnet(x, y)
fit$beta[, fit$df == 7]
x <- matrix(rnorm(100000), 100, 1000)
y = x %*% c(rep(0.2,7), rep(0,993)) + rnorm(100)
fit = glmnet(x, y)
fit$beta
fit$df
fit = glmnet(x, y, nlambda = 1000)
fit$beta[, fit$df == 7]
fit$\
fit$df
fit$beta[, fit$df == 7]
fit$df == 7
sum(fit$df == 7)
B = fit$beta
dim(B)
(1:896)[fit$df == 7]
B[, 71]
library(ncvreg)
fit <- ncvreg(x, y, penalty = "SCAD")
fit$beta
fit$alpha
fit$gamma
fit$penalty
fit$iter
fit$lambda
x <- matrix(rnorm(100000), 100, 100)
y = x %*% c(rep(1,7), rep(0,93)) + rnorm(100)
fit <- ncvreg(x, y, penalty = "SCAD")
fit$iter
fit$beta
fit$beta[,100]
sum( fit$beta[,100] != 0 )
fit$family
fit$n
fit$lambda
fit$loss
fit$convex.min
fit$penalty.factor
plot(fit)
mvtnorm::rmvnorm(n, mean = rep(0, 10))
r = 0.3
cov_mat <- (1-r) * diag(p) + r * matrix(1, p, p)
cov_mat
?mvtnorm::rmvnorm
>ncvreg
?ncvreg
fit_scad <- ncvreg(x, y, penalty = "SCAD", nlambda = 100)
fit_lasso <- glmnet(x, y, nlambda = 100)
fit_scad$penalty.factor
fit_scad$lambda
fit_scad$beta
coef(fit_scad)
coef(fit_scad, lambda = 0.5)
coef(fit_scad, df = 7)
?ncvreg
zz = coef(fit_scad, df = 7)
dim(zz)
cv.ncvreg(x, y)
zz = cv.ncvreg(x, y, penalty = "SCAD")
zz$min
zz$lambda.min
zz$fold
zz$fit$penalty
zz$fit$lambda[61]
zz = cv.glmnet(x,y)
zz$lambda.min
zz$lambda
zz$name
zz$cvm
zz$lambda
zz$nzero
zz$lambda.1se
zz$lambda.min
?dlmnet
?glmnet
p_active <- sum(b0 != 0)
p
p_active
fit_scad <- ncvreg(x, y, penalty = "SCAD", nlambda = 100, dfmax = p_active + 1)
fit_scad
fit_scad$beta
dim(fit_scad$beta)
cv_scad <- cv.ncvreg(x, y, penalty = "SCAD", nlambda = 100)
cv_lasso <- cv.glmnet(x, y, nlambda = 100)
fit_scad <- cv_scad$fit$beta
fit_scad
fit_lasso <- cv_lasso$glmnet.fit$beta
fit_lasso
fit_scad
apply(fit_scad!=0, 2, sum)
apply(fit_scad[-1, ] != 0, 2, sum)
# Homework 4 DSO 607
library(glmnet)
library(ncvreg)
# Monte Carlo Setup
n <- 100
p <- 1000
b0 <- c(1, -0.5, 0.7, -1.2, -0.9, 0.3, 0.55)
b0 <- c(b0, rep(0, p - length(b0)))
p_active <- sum(b0 != 0)
sigma0 <- 0.3
Rep <- 100
r_range <- c(0, 0.25, 0.5, 0.75)
correct_ratio_df <- matrix(0, Rep, length(r_range))
correct_ratio_cv <- matrix(0, Rep, length(r_range))
dgp <- function(n, p, b0, sigma0, r){
cov_mat <- (1-r) * diag(p) + r * matrix(1, p, p)
x <- mvtnorm::rmvnorm(n, mean = rep(0, p), sigma = cov_mat)
y <- x %*% b0 + (rnorm(n)*sigma0)
return(list(x = x, y = y))
}
r = 0
data <- dgp(n, p, b0, sigma0, r)
x <- data$x
y <- data$y
cv_scad <- cv.ncvreg(x, y, penalty = "SCAD", nlambda = 100)
cv_lasso <- cv.glmnet(x, y, nlambda = 100)
fit_scad <- cv_scad$fit$beta[-1, ]
fit_lasso <- cv_lasso$glmnet.fit$beta
apply(fit_lasso != 0, 2, sum)
apply(fit_scad != 0, 2, sum)
fit_scad[1:10, apply(fit_scad != 0, 2, sum) == 7]
fit_scad[1:10, apply(fit_scad != 0, 2, sum) == 7]
model_select <- function(b, b0, lambda, lambda_min) {
p_active <- sum(b0 != 0)
p <- length(b0)
df_est <- apply(b != 0, 2, sum)
b_cv <- b[, lambda == lambda_min]
b_df <- b[, df_est == p_active]
correct_cv <- (sum((b_cv != 0) == (b0 != 0)) == p)
correct_df <- (sum((b_df[, 1] != 0) == (b0 != 0)) == p)
return(list(cv = correct_cv, df = correct_df))
}
cv_scad$lambda
cv_scad$lambda.min
(1:length(cv_scad))[cv_scad$lambda == cv_scad$lambda.min]
(1:length(cv_scad$lambda))[cv_scad$lambda == cv_scad$lambda.min]
cv_scad$min
cv_lasso$lambda
cv_lasso$lambda.min
rm(list = ls())
source('C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.R')
rm(list = ls())
source('C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.R')
source('C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.R')
rm(list = ls())
source('C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.R')
correct_ratio_df
source('C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.R')
b = cv_scad$fit$beta[-1, ]
df_est <- apply(b != 0, 2, sum)
df_est
b = cv_lasso$glmnet.fit$beta
df_est <- apply(b != 0, 2, sum)
df_est
b_df <- as.matrix(b[, df_est == p_active])
b_df
source('C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.R')
apply(correct_ratio_cv, c(2,3), mean)
apply(correct_ratio_df, c(2,3), mean)
correct_ratio_df <- cbind(r_range, apply(correct_ratio_df, c(2,3), mean))
correct_ratio_df
correct_ratio_df <- cbind(r_range, apply(correct_ratio_df, c(2,3), mean))
colnames(correct_ratio_df) <- c("r", "SCAD", "Lasso")
correct_ratio_cv <- cbind(r_range, apply(correct_ratio_cv, c(2,3), mean))
colnames(correct_ratio_cv) <- c("r", "SCAD", "Lasso")
colnames(correct_ratio_df) <- c("r", "SCAD", "Lasso")
correct_ratio_cv <- cbind(r_range, apply(correct_ratio_cv, c(2,3), mean))
correct_ratio_df
correct_ratio_cv
save.image("C:/Users/zhang/Dropbox/CourseworkUSC/DSO607/homework 4/hw4.RData")
library(classo)
# Test
#
data("sample_data")
y <- as.matrix(sample_data$Y)
X <- as.matrix(sample_data[, -1])
x = X
lambda <- 0.5 * var(y) * 25^(-1/3)
N = 200
TT = 25
K = 3
beta0 = NULL
p <- dim(X)[2]
if (is.null(beta0)) {
# Use individual regression result as the initial value
beta0 <- init_est(X, y, TT)
}
b.out <- array(beta0, c(N, p, K))
a.out <- matrix(0, K, p)
b.old <- matrix(1, N, p)
a.old <- matrix(1, 1, p)
r = 1
k = 1
# N * 1: consider it as gamma
gamma <- pen.generate(b.out, a.out, N, p, K, k)
# Commented out: Suggested by Dr. Narasimhan b = Variable(N*p) a =
# Variable(p) obj = 0 End Commented out
X.list = list()
for (i in 1:N) {
ind = ((i - 1) * TT + 1):(i * TT)
id = ((i - 1) * p + 1):(i * p)
X.list[[i]] = X[ind, ]
# Commented out: Suggested by Dr. Narasimhan obj = obj + gamma[i] *
# norm2( b[id] - a ) End Commented out
}
## Code added
b = Variable(p, N)
a = Variable(p)
A <- matrix(1, nrow = 1, ncol = N)
obj1 <- t(norm2(b - a %*% A, axis = 2)) %*% gamma
## End Code added
XX = bdiag(X.list)
## Original commented out obj = Minimize( sum_squares(y - XX %*% b)/(N
## * TT) + obj*(lambda/N) ) End Original commented out
## Code added and modified
obj = Minimize(sum_squares(y - XX %*% vec(b))/(N * TT) +
obj1 * (lambda/N))
## End Code added and modified
Prob = Problem(obj)
pen.generate <- function(b, a, N, p, K, kk) {
# generate the known part of the penalty term output a N*1 vector
a.out.exp <- aperm(array(a, c(K, p, N)), c(3, 2, 1))
p.temp <- (b - a.out.exp)^2
p.norm <- sqrt(apply(p.temp, c(1, 3), sum))
ind <- setdiff(1:K, kk)
pen <- apply(as.matrix(p.norm[, ind]), 1, prod)
return(pen)
}
# N * 1: consider it as gamma
gamma <- pen.generate(b.out, a.out, N, p, K, k)
# Commented out: Suggested by Dr. Narasimhan b = Variable(N*p) a =
# Variable(p) obj = 0 End Commented out
X.list = list()
for (i in 1:N) {
ind = ((i - 1) * TT + 1):(i * TT)
id = ((i - 1) * p + 1):(i * p)
X.list[[i]] = X[ind, ]
# Commented out: Suggested by Dr. Narasimhan obj = obj + gamma[i] *
# norm2( b[id] - a ) End Commented out
}
## Code added
b = Variable(p, N)
a = Variable(p)
A <- matrix(1, nrow = 1, ncol = N)
obj1 <- t(norm2(b - a %*% A, axis = 2)) %*% gamma
## End Code added
XX = bdiag(X.list)
## Original commented out obj = Minimize( sum_squares(y - XX %*% b)/(N
## * TT) + obj*(lambda/N) ) End Original commented out
## Code added and modified
obj = Minimize(sum_squares(y - XX %*% vec(b))/(N * TT) +
obj1 * (lambda/N))
## End Code added and modified
Prob = Problem(obj)
solver = "ECOS"
cvxr.out = solve(Prob, solver = solver)
cvxr.out$getValue(a)
matrix(cvxr.out$getValue(b), N, p, byrow = TRUE)
cvxr.out$getValue(b)
library(classo)
data("sample_data")
# CAVEAT: Please convert data.frame to matrix to proceed.
y <- as.matrix(sample_data[, 1])
x <- as.matrix(sample_data[, -1])
n <- 200
tt <- 25
lambda <- as.numeric( 0.5 * var(y) / (tt^(1/3)) )
pls_out <- PLS.cvxr.solver(n, tt, y, x, K = 3, lambda = lambda)
library(classo)
data("sample_data")
# CAVEAT: Please convert data.frame to matrix to proceed.
y <- as.matrix(sample_data[, 1])
x <- as.matrix(sample_data[, -1])
n <- 200
tt <- 25
lambda <- as.numeric( 0.5 * var(y) / (tt^(1/3)) )
pls_out <- PLS.cvxr.solver(n, tt, y, x, K = 3, lambda = lambda)
pls_out <- PLS.cvxr.solver(n, tt, y, x, K = 3, lambda = lambda)
PLS.cvxr <- function(N, TT, y, X, K, lambda, R, tol = 1e-4, solver = "ECOS"){
# library("Rmosek")
# library("SparseM")
# library("Matrix")
# library("reticulate")
# PLS estimation by the iterative algorithm
# INPUT Arg:
#   dimensions N, TT
#   data y(TN * 1), X(TN * P)
#   tuning parameter lambda
#   maximum number of iteration R
# OUTPUT:
#   b_est: estimated beta (N*p)
#   a_out: estimated alpha (K*p)
#   group_est: estimated group identity (N*1)
p <- dim(X)[2];
# Use individual regression result as the initial value
beta0 = matrix(0, N, p);
for(i in 1:N){
ind <- ( (i-1)*TT+1 ):(i*TT);
yy <- y[ind, ]
XX <- X[ind, ]
beta0[i, ] <- solve( t(XX) %*% XX ) %*% ( t(XX) %*% yy );
}
b.out <- array( beta0, c(N,p,K) );
a.out <- matrix(0,K,p);
b.old <- matrix(1,N,p);
a.old <- matrix(1,1,p);
for(r in 1:R){
for(k in 1:K){
# print(c(r,k))
# N * 1: consider it as gamma
gamma <- pen.generate(b.out, a.out, N, p, K, k)
# Commented out: Suggested by Dr. Narasimhan
# b = Variable(N*p)
# a = Variable(p)
#
# obj = 0
# End Commented out
X.list = list()
for(i in 1:N){
ind = ( (i-1)*TT + 1 ):(i*TT)
id = ( (i-1)*p + 1 ):(i*p)
X.list[[i]] =  X[ind, ]
# Commented out: Suggested by Dr. Narasimhan
# obj = obj + gamma[i] * norm2( b[id] - a )
# End Commented out
}
## Code added
b = Variable(p, N)
a = Variable(p)
A <- matrix(1, nrow = 1, ncol = N)
obj1 <- norm2(b - a %*% A, axis = 2) %*% gamma
## End Code added
XX = bdiag(X.list)
## Original commented out
## obj = Minimize( sum_squares(y - XX %*% b)/(N * TT) + obj*(lambda/N) )
## End Original commented out
## Code added and modified
obj = Minimize( sum_squares(y - XX %*% vec(b))/(N * TT) + obj1*(lambda/N) )
## End Code added and modified
Prob = Problem(obj)
cvxr.out = solve(Prob, solver = solver)
a.out[k, ] = cvxr.out$getValue(a)
b.out[ , , k] = matrix( cvxr.out$getValue(b), N, p, byrow = TRUE)
}
# Check the convergence criterion
a.new <- a.out[K,];
b.new <- b.out[ , ,K];
if(criterion(a.old,a.new,b.old,b.new,tol)){
break;
}
# Update
a.old <- a.out[K,];
b.old <- b.out[ , ,K];
}
# put b.out to nearest a.out and get the group estimation
a.out.exp <- aperm( array(a.out, c(K,p,N)), c(3,2,1) );
d.temp <- (b.out - a.out.exp)^2;
dist <- sqrt( apply(d.temp, c(1,3), sum) );
group.est <- apply(dist,1,which.min);
if( sum( as.numeric( table(group.est) ) > p ) == K ){
# Post-Lasso estimation
a.out <- post.lasso( group.est, y, X, K, p, N, TT );
}
b.est <- matrix(999, N, p);
for(i in 1:N){
group <- group.est[i];
b.est[i,] <- a.out[group, ];
}
result <- list( b.est = b.est, a.out = a.out,
group.est = group.est );
return(result)
}
x <- rnorm(100)
y <- rnorm(100)
plot(x,y)
library("Rmosek")
library("SparseM")
library("Matrix")
library("CVXR")
setwd("C:/Users/zhang/Desktop/share_code")
source("PLS_est.R")
source("tools_func.R")
source('C:/Users/zhang/Desktop/share_code/master.R')
source('C:/Users/zhang/Desktop/share_code/master.R')
source('C:/Users/zhang/Desktop/share_code/master.R')
rm(list = ls())
sessionInfo()
setwd("C:/Users/zhang/Desktop/classo_code")
source('C:/Users/zhang/Desktop/classo_code/master.R')
